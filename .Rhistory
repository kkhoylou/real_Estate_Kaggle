text(tree.oj)
tree.oj
text(tree.oj,pretty-0)
text(tree.oj,pretty=0)
pred.oj <- predict(tree.oj,test)
table(pred.oj,test$Purchase)
View(pred.oj)
pred.oj1 <- max(pred.oj[,1],pred.oj[,2])
View(pred.oj)
pred.oj1 <- ifelse(pred.oj[,1]>pred.oj[,2],pred.oj[,1],pred.oj[,2])
pred.oj1 <- ifelse(pred.oj[,1]>pred.oj[,2],"CH","MM")
table(pred.oj,test$Purchase)
View(test)
table(pred.oj1,test$Purchase)
cv.oj <- cv.tree(tree.oj)
summary(cv.oj)
cv.oj
plot(cv.oj$size ,cv.oj$dev,type = 'b')
plot(cv.oj$k ,cv.oj$dev,type = 'b')
par(1,1,2)
plot(cv.oj$size ,cv.oj$dev,type = 'b')
plot(cv.oj$k ,cv.oj$dev,type = 'b')
plot(cv.oj$size ,cv.oj$dev,type = 'b')
prune.oj <- prune.tree(tree.oj,best = 7)
plot(prune.oj)
test(prune.oj,pretty=0)
text(prune.oj,pretty=0)
pred.prune <- pred(prune.oj,test)
pred.prune <- predict(prune.oj,test)
View(pred.prune)
pred.oj2 <- ifelse(pred.prune[,1]>pred.prune[,2],"CH","MM")
table(pred.oj2,test$Purchase)
rm(list=ls())
Hitters <- Hitters
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train.Hitters <- Hitters[1:200,]
test.Hitters <- Hitters[201:263,]
library(gbm)
install.packages("gym")
install.packages("gbm")
library(gbm)
set.seed(9)
set.seed(90)
boost.Hitters <- gmb(Salary~.,train.Hittersdistribution="gaussian ",n.trees=1000)
boost.Hitters <- gbm(Salary~.,train.Hittersdistribution="gaussian ",n.trees=1000)
boost.Hitters <- gbm(Salary~.,train.Hittersdistribution="gaussian",n.trees=1000)
boost.Hitters <- gbm(Salary~.,train.Hitters,distribution="gaussian",n.trees=1000)
summary(boost.Hitters)
pred.boost <- predict(boost.Hitters,test.Hitters)
pred.boost <- predict(boost.Hitters,test.Hitters,n.trees=1000)
mean((pred.boost-test.Hitters$Salary)^2)
lm.fit <- lm(Salary~.,train.Hitters)
lm.pred <- predict(lm.fit,test.Hitters)
mean((lm.pred-test.Hitters$Salary)^2)
set.seed(9)
bag.Hitters <- randomForest(Salary~.,train.Hitters)
pred.bag <- predict(bag.Hitters,test.Hitters)
mean((pred.bag-test.Hitters)^2)
mean((pred.bag-test.Hitters$Salary)^2)
rm(list=ls())
Caravan <- Caravan
train.Caravan <- Caravan[1:1000,]
test.Caravan <- Caravan[1001:5822,]
boost.Caravan <- gbm(Purchase~.,train.Caravan,n.trees=1000,shrinkage = .01)
boost.Caravan <- gbm(Purchase~.,data=train.Caravan,n.trees=1000,shrinkage = .01)
boost.Caravan <- gbm(Purchase~.,data=train.Caravan,distribution="gaussian",n.trees=1000,shrinkage = .01)
summary(boost.Caravan)
boost.predict <- predict(boost.Caravan,test.Caravan)
boost.predict <- predict(boost.Caravan,test.Caravan,n.trees=1000)
mean((boost.predict-test.Caravan$Purchase)^2)
boost.predict <- ifelse(boost.predict > 0.2, 1, 0)
mean((boost.predict-test.Caravan$Purchase)^2)
table(boost.predict,test.Caravan$Purchase)
boost.predict <- predict(boost.Caravan,test.Caravan,n.trees=1000)
boost.predict <- predict(boost.Caravan,test.Caravan,n.trees=1000,type="response")
boost.predict <- ifelse(boost.predict > 0.2, 1, 0)
table(boost.predict,test.Caravan$Purchase)
boost.predict <- predict(boost.Caravan,test.Caravan,n.trees=1000,type="response")
boost.prob <- ifelse(boost.predict > 0.2, 1, 0)
table(boost.prob,test.Caravan$Purchase)
boost.prob <- ifelse(boost.predict > 0.2, "Yes", "No")
table(boost.prob,test.Caravan$Purchase)
boost.predict <- predict(boost.Caravan,test.Caravan,n.trees=1000,type="response")
boost.prob <- ifelse(boost.predict > 0.2, "Yes", "No")
table(boost.prob,test.Caravan$Purchase)
table(data.frame(boost.prob),test.Caravan$Purchase)
rm(list=ls())
library(MASS)
library(ISLR)
library(dplyr)
library(randomForest)
library(tree)
library(gbm)
Boston <- Boston
train <- sample(506,258)
test <- Boston[-train,]
train <- Boston[train,]
set.seed(1)
rm(list=ls())
Wage <- Wage
train <- sample(506,258)
test <- Wage[-train,]
train <- Wage[train,]
#boosting
set.seed(1)
boost.Wage <- gbm(wage~.,distribution = "gaussian",n.trees=5000,
interaction.depth = 4)
boost.Wage <- gbm(wage~.,train,distribution = "gaussian",n.trees=5000,
interaction.depth = 4)
summary(boost.Wage)
boost.Wage <- gbm(wage~.-logwage,train,distribution = "gaussian",n.trees=5000,
interaction.depth = 4)
summary(boost.Wage)
boost.predict <- predict(boost.Wage,test)
boost.predict <- predict(boost.Wage,test,n.trees=5000)
mean((boost.predict-test$wage)^2)
train <- sample(3000,1500)
test <- Wage[-train,]
train <- Wage[train,]
#boosting
set.seed(1)
boost.Wage <- gbm(wage~.-logwage,train,distribution = "gaussian",n.trees=5000,
interaction.depth = 4)
summary(boost.Wage)
boost.predict <- predict(boost.Wage,test,n.trees=5000)
mean((boost.predict-test$wage)^2)
train <- sample(2000,1000)
test <- Wage[-train,]
train <- Wage[train,]
set.seed(1)
boost.Wage <- gbm(wage~.-logwage,train,distribution = "gaussian",n.trees=5000,
interaction.depth = 4)
summary(boost.Wage)
boost.predict <- predict(boost.Wage,test,n.trees=5000)
mean((boost.predict-test$wage)^2)
boost.Wage <- gbm(wage~.-logwage,train,distribution = "gaussian",n.trees=5000,
interaction.depth = 4,shrinkage = .2)
summary(boost.Wage)
boost.predict <- predict(boost.Wage,test,n.trees=5000)
mean((boost.predict-test$wage)^2)
boost.Wage <- gbm(wage~.-logwage,train,distribution = "gaussian",n.trees=5000,
interaction.depth = 4,shrinkage = .01)
boost.predict <- predict(boost.Wage,test,n.trees=5000)
mean((boost.predict-test$wage)^2)
p <- dim(Wage)
p <- dim(Wage)[2]-1
p.r <- sqrt(p)
rf.p.Wage <- randomForest(wage~.-logwage,train,mtry=p,
importance =TRUE)
rf.p.2.Wage <- randomForest(wage~.-logwage,train,mtry=p.2,
importance =TRUE)
p.2 <- p/2
rf.p.2.Wage <- randomForest(wage~.-logwage,train,mtry=p.2,
importance =TRUE)
rf.p.r.Wage <- randomForest(wage~.-logwage,train,mtry=p.r,
importance =TRUE)
rf.p.Wage <- randomForest(wage~.-logwage,train,mtry=p,
importance =TRUE,ntree=500)
rf.p.2.Wage <- randomForest(wage~.-logwage,train,mtry=p.2,
importance =TRUE,ntree=500)
rf.p.r.Wage <- randomForest(wage~.-logwage,train,mtry=p.r,
importance =TRUE,ntree=500)
plot(1:500,rf.p.Wage$test$mse,col='blue',type='l')
lines(1:500,rf.p.2.Wage$test$mse,col='red',type='l')
lines(1:500,rf.p.r.Wage$test$mse,col='green',type='l')
View(train)
rf.p.Wage <- randomForest(wage~.-logwage,train[,-12],train[,12],
test[,-12],test[,12],mtry=p,
importance =TRUE,ntree=500)
rf.p.Wage <- randomForest(train[,-12],train[,12],
xtest=test[,-12],ytest=test[,12],mtry=p,
importance =TRUE,ntree=500)
rf.p.Wage <- randomForest(train[,-c(11,12)],train[,12],
xtest=test[,-c(11,12)],ytest=test[,12],
mtry=p,importance =TRUE,ntree=500)
plot(1:500,rf.p.Wage$test$mse,col='blue',type='l')
rf.p.2.Wage <- randomForest(train[,-c(11,12)],train[,12],
xtest=test[,-c(11,12)],ytest=test[,12],
mtry=p.2,importance =TRUE,ntree=500)
rf.p.r.Wage <- randomForest(train[,-c(11,12)],train[,12],
xtest=test[,-c(11,12)],ytest=test[,12],
mtry=p.r,importance =TRUE,ntree=500)
plot(1:500,rf.p.Wage$test$mse,col='blue',type='l')
lines(1:500,rf.p.2.Wage$test$mse,col='red',type='l')
lines(1:500,rf.p.r.Wage$test$mse,col='green',type='l')
min(rf.p.r.Wage$test$mse)
min(rf.p.2.Wage$test$mse)
set.seed(1)
boost.Wage <- gbm(wage~.-logwage,train,distribution = "gaussian",n.trees=5000,
interaction.depth = 4)
summary(boost.Wage)
boost.predict <- predict(boost.Wage,test,n.trees=5000)
mean((boost.predict-test$wage)^2)
rf.p.Wage
p <- dim(Wage)[2]-2
p.2 <- p/2
p.r <- sqrt(p)
rf.p.Wage <- randomForest(train[,-c(11,12)],train[,12],
xtest=test[,-c(11,12)],ytest=test[,12],
mtry=p,importance =TRUE,ntree=500)
rf.p.2.Wage <- randomForest(train[,-c(11,12)],train[,12],
xtest=test[,-c(11,12)],ytest=test[,12],
mtry=p.2,importance =TRUE,ntree=500)
rf.p.r.Wage <- randomForest(train[,-c(11,12)],train[,12],
xtest=test[,-c(11,12)],ytest=test[,12],
mtry=p.r,importance =TRUE,ntree=500)
plot(1:500,rf.p.Wage$test$mse,col='blue',type='l')
lines(1:500,rf.p.2.Wage$test$mse,col='red',type='l')
lines(1:500,rf.p.r.Wage$test$mse,col='green',type='l')
min(rf.p.r.Wage$test$mse)
min(rf.p.2.Wage$test$mse)
rf.p.Wage
library(rworldmap)
library(ggmap)
library(mapproj)
library(dplyr)
library(readr)
library(ggplot2)
library(stringr)
library(rMap)
library(devtools)
library(leaflet)
library(shiny)
#Clear Environment
rm(list=ls())
sd_Cal <- read.csv("calendar_D.csv",header = T)
sd_Listings <- read.csv("listings_D.csv",header = T)
sd_Reviews <- read.csv("reviews_D.csv",header = T)
#Clean up listings data
set.seed(1)
x <- matrix(rnorm(100), ncol=2)
x <- matrix(rnorm(100*2), ncol=2)
library(e1071)
y <- c(rep(-1,50), rep(1,50))
x[y==1,] <- x[y==1,] + 1
plot(x, col=(3-y))
dat=data.frame(x=x, y=as.factor(y))
svmfit <- svm(yâˆ¼., data=dat, kernel="linear", cost=10,scale=FALSE)
svmfit <- svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE)
plot(svmfit,dat)
str(svmfit)
summary(svmfit)
svmfit=svm(y~., data=dat, kernel="linear", cost=0.1, scale=FALSE)
plot(svmfit)
plot(svmfit, dat)
svmfit$index
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out$best.model
summary(tune.out)
tune.out.linear <- tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out.poly <- tune(svm,y~.,data=dat,kernel="polynomial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out.radial <- tune(svm,y~.,data=dat,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out.linear$best.model
summary(tune.out.linear)
summary(tune.out.polynomial)
summary(tune.out.poly)
summary(tune.out.radial)
bestmod.linear <- tune.out.linear$best.model
bestmod.poly <- tune.out.poly$best.model
bestmod.radial <- tune.out.radial$best.model
ypred.linear <- predict(bestmod.linear ,testdat)
ypred.radial <- predict(bestmod.radial ,testdat)
ypred.poly <- predict(bestmod.poly ,testdat)
library(e1071)
#4
set.seed(1)
x <- matrix(rnorm(100*2), ncol=2)
y <- c(rep(-1,50), rep(1,50))
x[y==1,] <- x[y==1,] + 1
plot(x, col=(3-y))
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE)
plot(svmfit,dat)
str(svmfit)
summary(svmfit)
#Using one svmfit with cost of 10 and kernel at linear
#Checking between tune outs between linear, poly, and radial kernels
tune.out.linear <- tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out.poly <- tune(svm,y~.,data=dat,kernel="polynomial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out.radial <- tune(svm,y~.,data=dat,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out.linear)
summary(tune.out.poly)
summary(tune.out.radial)
bestmod.linear <- tune.out.linear$best.model
bestmod.poly <- tune.out.poly$best.model
bestmod.radial <- tune.out.radial$best.model
#linear kernels with costs that are 1 or larger had the lowest error percentage
#create train and test
xtest <- matrix(rnorm(20*2), ncol=2)
ytest <- sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,] <- xtest [ ytest ==1 ,] + 1
testdat <- data.frame(x=xtest, y=as.factor(ytest))
ypred.linear <- predict(bestmod.linear ,testdat)
table(predict=ypred.linear, truth=testdat$y)
ypred.radial <- predict(bestmod.radial ,testdat)
table(predict=ypred.radial, truth=testdat$y)
ypred.poly <- predict(bestmod.poly ,testdat)
table(predict=ypred.poly, truth=testdat$y)
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)
xtest <- matrix(rnorm(100*2), ncol=2)
ytest <- sample(c(-1,1), 100, rep=TRUE)
xtest[ytest==1,] <- xtest [ ytest ==1 ,] + 1
testdat <- data.frame(x=xtest, y=as.factor(ytest))
ypred.linear <- predict(bestmod.linear ,testdat)
table(predict=ypred.linear, truth=testdat$y)
ypred.radial <- predict(bestmod.radial ,testdat)
table(predict=ypred.radial, truth=testdat$y)
ypred.poly <- predict(bestmod.poly ,testdat)
table(predict=ypred.poly, truth=testdat$y)
table(predict=ypred.poly, truth=-testdat$y)
table(predict=ypred.poly, truth=-1*testdat$y)
table(predict=ypred.poly, truth=(-1*testdat$y))
table(predict=ypred.poly, truth=(-1*numeric(testdat$y)))
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)
plot(ypred.poly
)
rm(list=ls())
x1 <- runif(500)-0.5
x1
x2=runif(500)-0.5
y <- 1*(x1^2-x2^2 > 0)
y <- (x1^2-x2^2 > 0)
y <- 1*(x1^2-x2^2 > 0)
(x2,x1,col=y)
plot(x2,x1,col=y)
plot(x2,x1,color=y)
plot(x2,x1)
plot(x2,x1)
plot(x2,x1)
plot(x2,x1,col=y)
lm.fit <- lm(y~x1+x2)
library(e1071)
x1 <- runif(500)-0.5
x2 <- runif(500)-0.5
y <- 1*(x1^2-x2^2 > 0)
#B
plot(x2,x1,col=y)
#c
lm.fit <- lm(y~x1+x2)
summary(lm.fit)
lm.fit <- glm(y~x1+x2)
summary(lm.fit)
lm.fit <- glm(y~x1+x2,family = binomial)
summary(lm.fit)
train <- sample(500,250)
test <- data.frame(x1[-train,],x2[-train,],y[-train,])
test <- data.frame(x1[-train],x2[-train],y[-train])
train <- data.frame(x1[train],x2[train],y[train])
pred.train <- predict(lm.fit,train)
plot(pred.train
)
plot(pred.train,col=y)
glm.fit <- glm(y~I(x1^2-x2^2))
library(e1071)
x1 <- runif(500)-0.5
x2 <- runif(500)-0.5
y <- 1*(x1^2-x2^2 > 0)
#B
plot(x2,x1,col=y)
#c
glm.fit <- glm(y~x1+x2,family = binomial)
summary(glm.fit)
pred.train <- predict(glm.fit,train)
plot(pred.train,col=y)
train <- sample(500,250)
test <- data.frame(x1[-train],x2[-train],y[-train])
train <- data.frame(x1[train],x2[train],y[train])
pred.train <- predict(glm.fit,train)
plot(pred.train,col=y)
glm.fit <- glm(y~I(x1^2-x2^2))
glm.fit <- glm(y~x1+x2,family = binomial)
glm2.fit <- glm(y~I(x1^2-x2^2))
summary(glm2.fit)pred.train <- predict(glm2.fit,train)
summary(glm2.fit)
glm2.fit <- glm(y~I(x1^2-x2^2)+x2^2)
summary(glm2.fit)
glm2.fit <- glm(y~I(x1^2-x2)+x2^2)
summary(glm2.fit)
pred.train <- predict(glm2.fit,train)
pred.train <- predict(glm2.fit,train)
plot(pred.train,col=y)
set.seed(1)
dat=data.frame(x=c(x1,x2), y=as.factor(y))
dat <- data.frame(x1,x2, y=as.factor(y))
tune.out.radial <- tune(svm,y~.,data=dat,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out.radial)
svm.fit <- tune.out.radial$best.model
set.seed(1)
tune.out.radial <- tune(svm,y~.,data=train,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
View(train)
tune.out.radial <- tune(svm,y.train~.,data=train,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out.radial <- tune(svm,y.train.~.,data=train,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out.radial)
svm.fit <- tune.out.radial$best.model
pred.svm <- predict(svm.fit,test)
View(train)
View(test)
test <- data.frame(x1[-train],x2[-train],y[-train])
train <- data.frame(x1[train],x2[train],y[train])
colnames(train) <- c("x1","x2","x3")
colnames(test) <- c("x1","x2","x3")
set.seed(1)
tune.out.radial <- tune(svm,y.train.~.,data=train,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out.radial <- tune(svm,y~.,data=train,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
colnames(train) <- c("x1","x2","y")
colnames(test) <- c("x1","x2","y")
set.seed(1)
tune.out.radial <- tune(svm,y~.,data=train,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out.radial)
set.seed(5)
tune.out.radial <- tune(svm,y~.,data=train,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out.radial)
svm.fit <- tune.out.radial$best.model
pred.svm <- predict(svm.fit,test)
plot(pred.svm)
plot(pred.svm,col=y)
tune.out.radial <- tune(svm,y~.,data=train,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out.radial)
tune.out.radial <- tune(svm,y~.,data=train,kernel="radial",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out.radial)
svm.fit <- tune.out.radial$best.model
pred.svm <- predict(svm.fit,test)
plot(pred.svm,col=y)
set.seed(1)
x <- matrix(rnorm(200*2),ncol=2)
x[1:75,] <- x[1:75,]+2
x[76:150,] <- x[76:150,]-2
y <- c(rep(1,150),rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))
plot(x, col=y)
train <- sample(200,100)
tune.out=tune(svm,y~.,data=dat[train,],kernel="radial",
ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
library(e1071)
tune.out <- tune(svm,y~.,data=dat[train,],kernel="radial",
ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
test <- dat[-train,]
train <- dat[train,]
pred.svm <- predict(tune.out$best.model,test)
plot(pred.svm,col=y)
table(pred.svm,test$y)
average(pred.svm==test$y)
mean(pred.svm==test$y)
tune.out <- tune(svm,y~.,train[final.train],ranges=list(cost=c(.01,0.1,1,10)))
rm(list=ls())
OJ <- OJ
#A
train <- sample(1070,800)
library(e1071)
library(ISLR)
rm(list=ls())
OJ <- OJ
#A
train <- sample(1070,800)
test <- OJ[-train,]
train <- OJ[train,]
#B
svm.fit <- svm(Purchase~.,train,cost=.01)
summary(svm.fit)
#C
mean((predict(svm.fit,train)!=train$Purchase)^2)#.385
mean((predict(svm.fit,test)!=test$Purchase)^2)#.403
final.train <- sample(800,100)
tune.out <- tune(svm,y~.,train[final.train],ranges=list(cost=c(.01,0.1,1,10)))
tune.out <- tune(svm,y~.,train[final.train,],ranges=list(cost=c(.01,0.1,1,10)))
tune.out <- tune(svm,y~.,train[final.train,],ranges=list(cost=c(.01,0.1,1,5,10)))
rm(list=ls())
plot(x2[y==0],x1[y==0],col='red'pch='+')
plot(x2[y==0],x1[y==0],col='red',pch='+')
points(x2[y==1],x1[y==1],col='blue',pch='x')
rm(list=ls())
#A
x1 <- runif(500)-0.5
x2 <- runif(500)-0.5
y <- 1*(x1^2-x2^2 > 0)
#B
plot(x2[y==0],x1[y==0],col='red',pch='+')
points(x2[y==1],x1[y==1],col='blue',pch='x')
points(x2[y==1],x1[y==1],col='blue',pch='x')
train <- sample(500,250)
test <- data.frame(x1[-train],x2[-train],y[-train])
train <- data.frame(x1[train],x2[train],y[train])
colnames(train) <- c("x1","x2","y")
colnames(test) <- c("x1","x2","y")
pred.train <- predict(glm.fit,train,type='responsive')
glm.fit <- glm(y~x1+x2,family = binomial)
pred.train <- predict(glm.fit,train,type='responsive')
pred.train <- predict(glm.fit,train,type='response')
plot(pred.train,col=y)
prob.train <- predict(glm.fit,train,type='response')
pred.train <- ifelse(prob.train>.5149,1,0)
plot(pred.train,col=y)
pred.train <- ifelse(prob.train>.5199,1,0)
plot(pred.train,col=y)
library(rworldmap)
library(ggmap)
library(mapproj)
library(dplyr)
library(readr)
library(ggplot2)
library(stringr)
library(rMap)
library(devtools)
library(leaflet)
library(shiny)
red <- read.csv('Leb_Tower_Coord.csv')
library(ggplot2)
setwd("~/Desktop/real_Estate_Kaggle")
